{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Training and Evaluation\n",
    "===============================\n",
    "Train and evaluate: StaticMCDA, SimpleMLP, NeuralRanker, DINModel\n",
    "\n",
    "GPU Recommended for neural baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Setup\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "os.makedirs('/kaggle/working/results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Load Data\n",
    "# ============================================================\n",
    "DATA_DIR = '/kaggle/input/docmatchnet-data'\n",
    "\n",
    "doctor_embeddings = torch.load(f'{DATA_DIR}/doctor_embeddings.pt')\n",
    "case_embeddings = torch.load(f'{DATA_DIR}/case_embeddings.pt')\n",
    "clinical_features = torch.load(f'{DATA_DIR}/clinical_features.pt')\n",
    "pastwork_features = torch.load(f'{DATA_DIR}/pastwork_features.pt')\n",
    "logistics_features = torch.load(f'{DATA_DIR}/logistics_features.pt')\n",
    "trust_features = torch.load(f'{DATA_DIR}/trust_features.pt')\n",
    "context_features = torch.load(f'{DATA_DIR}/context_features.pt')\n",
    "relevance_labels = torch.load(f'{DATA_DIR}/relevance_labels.pt')\n",
    "doctor_indices = torch.load(f'{DATA_DIR}/doctor_indices.pt')\n",
    "splits = torch.load(f'{DATA_DIR}/splits.pt')\n",
    "case_metadata = torch.load(f'{DATA_DIR}/case_metadata.pt')\n",
    "\n",
    "print('Loaded tensors successfully')\n",
    "print(f\"Train/Val/Test: {len(splits['train'])}/{len(splits['val'])}/{len(splits['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Datasets\n",
    "# ============================================================\n",
    "class PairwiseBaselineDataset(Dataset):\n",
    "    def __init__(self, indices, case_emb, doc_emb, doc_indices, clinical, pastwork, logistics, trust, context, relevance):\n",
    "        self.indices = indices\n",
    "        self.case_emb = case_emb\n",
    "        self.doc_emb = doc_emb\n",
    "        self.doc_indices = doc_indices\n",
    "        self.clinical = clinical\n",
    "        self.pastwork = pastwork\n",
    "        self.logistics = logistics\n",
    "        self.trust = trust\n",
    "        self.context = context\n",
    "        self.relevance = relevance\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        case_idx = self.indices[idx]\n",
    "        rel = self.relevance[case_idx]\n",
    "\n",
    "        pos_mask = rel >= 3\n",
    "        if pos_mask.sum() == 0:\n",
    "            pos_mask = rel == rel.max()\n",
    "\n",
    "        neg_mask = rel <= 1\n",
    "        if neg_mask.sum() == 0:\n",
    "            neg_mask = rel < rel.max()\n",
    "\n",
    "        pos_pool = torch.where(pos_mask)[0]\n",
    "        neg_pool = torch.where(neg_mask)[0]\n",
    "\n",
    "        pos_local = pos_pool[torch.randint(len(pos_pool), (1,))].item()\n",
    "        neg_local = neg_pool[torch.randint(len(neg_pool), (1,))].item()\n",
    "\n",
    "        pos_global = self.doc_indices[case_idx, pos_local]\n",
    "        neg_global = self.doc_indices[case_idx, neg_local]\n",
    "\n",
    "        return {\n",
    "            'case_embedding': self.case_emb[case_idx],\n",
    "            'pos_doctor_embedding': self.doc_emb[pos_global],\n",
    "            'neg_doctor_embedding': self.doc_emb[neg_global],\n",
    "            'pos_clinical': self.clinical[case_idx, pos_local],\n",
    "            'neg_clinical': self.clinical[case_idx, neg_local],\n",
    "            'pos_pastwork': self.pastwork[case_idx, pos_local],\n",
    "            'neg_pastwork': self.pastwork[case_idx, neg_local],\n",
    "            'pos_logistics': self.logistics[case_idx, pos_local],\n",
    "            'neg_logistics': self.logistics[case_idx, neg_local],\n",
    "            'pos_trust': self.trust[case_idx, pos_local],\n",
    "            'neg_trust': self.trust[case_idx, neg_local],\n",
    "            'context': self.context[case_idx]\n",
    "        }\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, indices, case_emb, doc_emb, doc_indices, clinical, pastwork, logistics, trust, context, relevance, metadata):\n",
    "        self.indices = indices\n",
    "        self.case_emb = case_emb\n",
    "        self.doc_emb = doc_emb\n",
    "        self.doc_indices = doc_indices\n",
    "        self.clinical = clinical\n",
    "        self.pastwork = pastwork\n",
    "        self.logistics = logistics\n",
    "        self.trust = trust\n",
    "        self.context = context\n",
    "        self.relevance = relevance\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        case_idx = self.indices[idx]\n",
    "        global_docs = self.doc_indices[case_idx]\n",
    "        return {\n",
    "            'case_embedding': self.case_emb[case_idx],\n",
    "            'doctor_embeddings': self.doc_emb[global_docs],\n",
    "            'clinical': self.clinical[case_idx],\n",
    "            'pastwork': self.pastwork[case_idx],\n",
    "            'logistics': self.logistics[case_idx],\n",
    "            'trust': self.trust[case_idx],\n",
    "            'context': self.context[case_idx],\n",
    "            'relevance': self.relevance[case_idx],\n",
    "            'context_category': self.metadata['context_category'][case_idx]\n",
    "        }\n",
    "\n",
    "train_ds = PairwiseBaselineDataset(\n",
    "    splits['train'], case_embeddings, doctor_embeddings, doctor_indices,\n",
    "    clinical_features, pastwork_features, logistics_features, trust_features,\n",
    "    context_features, relevance_labels\n",
    ")\n",
    "val_ds = EvalDataset(\n",
    "    splits['val'], case_embeddings, doctor_embeddings, doctor_indices,\n",
    "    clinical_features, pastwork_features, logistics_features, trust_features,\n",
    "    context_features, relevance_labels, case_metadata\n",
    ")\n",
    "test_ds = EvalDataset(\n",
    "    splits['test'], case_embeddings, doctor_embeddings, doctor_indices,\n",
    "    clinical_features, pastwork_features, logistics_features, trust_features,\n",
    "    context_features, relevance_labels, case_metadata\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f'Train batches: {len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Baseline Models\n",
    "# ============================================================\n",
    "class StaticMCDA:\n",
    "    def __init__(self):\n",
    "        self.weights = {'clinical': 0.40, 'pastwork': 0.25, 'logistics': 0.25, 'trust': 0.10}\n",
    "        self.clinical_weights = [0.55, 0.20, 0.15, 0.10]\n",
    "        self.pastwork_weights = [0.30, 0.25, 0.20, 0.15, 0.10]\n",
    "        self.logistics_weights = [0.30, 0.25, 0.20, 0.15, 0.10]\n",
    "        self.trust_weights = [0.50, 0.30, 0.20]\n",
    "\n",
    "    def score(self, clinical, pastwork, logistics, trust):\n",
    "        c_w = torch.tensor(self.clinical_weights, dtype=clinical.dtype, device=clinical.device)\n",
    "        p_w = torch.tensor(self.pastwork_weights, dtype=pastwork.dtype, device=pastwork.device)\n",
    "        l_w = torch.tensor(self.logistics_weights, dtype=logistics.dtype, device=logistics.device)\n",
    "        t_w = torch.tensor(self.trust_weights, dtype=trust.dtype, device=trust.device)\n",
    "\n",
    "        c_score = (clinical * c_w).sum(-1)\n",
    "        p_score = (pastwork * p_w).sum(-1)\n",
    "        l_score = (logistics * l_w).sum(-1)\n",
    "        t_score = (trust * t_w).sum(-1)\n",
    "\n",
    "        return self.weights['clinical'] * c_score + self.weights['pastwork'] * p_score + self.weights['logistics'] * l_score + self.weights['trust'] * t_score\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, embed_dim=384):\n",
    "        super().__init__()\n",
    "        input_dim = embed_dim + embed_dim + 4 + 5 + 5 + 3 + 8\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, patient_emb, doctor_emb, clinical, pastwork, logistics, trust, context):\n",
    "        x = torch.cat([patient_emb, doctor_emb, clinical, pastwork, logistics, trust, context], dim=-1)\n",
    "        return {'score': self.network(x)}\n",
    "\n",
    "class NeuralRanker(nn.Module):\n",
    "    def __init__(self, embed_dim=384, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.patient_proj = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.doctor_proj = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.feature_encoder = nn.Sequential(nn.Linear(25, 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.scorer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + 64, 128), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, patient_emb, doctor_emb, clinical, pastwork, logistics, trust, context):\n",
    "        p = self.patient_proj(patient_emb).unsqueeze(1)\n",
    "        d = self.doctor_proj(doctor_emb).unsqueeze(1)\n",
    "        interaction, _ = self.cross_attention(p, d, d)\n",
    "        interaction = self.layer_norm(interaction.squeeze(1))\n",
    "        feat = torch.cat([clinical, pastwork, logistics, trust, context], dim=-1)\n",
    "        feat_enc = self.feature_encoder(feat)\n",
    "        return {'score': self.scorer(torch.cat([interaction, feat_enc], dim=-1))}\n",
    "\n",
    "class DINModel(nn.Module):\n",
    "    def __init__(self, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.case_encoder = nn.Linear(embed_dim + 8, 128)\n",
    "        self.doctor_encoder = nn.Linear(embed_dim + 17, 128)\n",
    "        self.attention = nn.Sequential(nn.Linear(128 * 3, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "        self.scorer = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, patient_emb, doctor_emb, clinical, pastwork, logistics, trust, context):\n",
    "        case_enc = F.relu(self.case_encoder(torch.cat([patient_emb, context], dim=-1)))\n",
    "        doc_feat = torch.cat([clinical, pastwork, logistics, trust], dim=-1)\n",
    "        doc_enc = F.relu(self.doctor_encoder(torch.cat([doctor_emb, doc_feat], dim=-1)))\n",
    "        attn = torch.sigmoid(self.attention(torch.cat([case_enc, doc_enc, case_enc * doc_enc], dim=-1)))\n",
    "        return {'score': self.scorer(attn * doc_enc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Metrics and Evaluation\n",
    "# ============================================================\n",
    "def ndcg_at_k(scores, labels, k):\n",
    "    ranking = np.argsort(-scores)[:k]\n",
    "    dcg = sum((2**labels[r] - 1) / np.log2(i + 2) for i, r in enumerate(ranking))\n",
    "    ideal = np.argsort(-labels)[:k]\n",
    "    idcg = sum((2**labels[r] - 1) / np.log2(i + 2) for i, r in enumerate(ideal))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def evaluate_model(model_or_mcda, dataloader, device, is_mcda=False):\n",
    "    if not is_mcda:\n",
    "        model_or_mcda.eval()\n",
    "\n",
    "    ndcg5, ndcg10 = [], []\n",
    "    context_res = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating', leave=False):\n",
    "            case_emb = batch['case_embedding'].to(device)\n",
    "            docs = batch['doctor_embeddings'].squeeze(0).to(device)\n",
    "            clinical = batch['clinical'].squeeze(0).to(device)\n",
    "            pastwork = batch['pastwork'].squeeze(0).to(device)\n",
    "            logistics = batch['logistics'].squeeze(0).to(device)\n",
    "            trust = batch['trust'].squeeze(0).to(device)\n",
    "            context = batch['context'].to(device)\n",
    "            labels = batch['relevance'].squeeze(0).numpy()\n",
    "            ctx = batch['context_category'][0]\n",
    "\n",
    "            scores = []\n",
    "            for i in range(docs.shape[0]):\n",
    "                if is_mcda:\n",
    "                    s = model_or_mcda.score(\n",
    "                        clinical[i:i+1], pastwork[i:i+1], logistics[i:i+1], trust[i:i+1]\n",
    "                    ).item()\n",
    "                else:\n",
    "                    s = model_or_mcda(\n",
    "                        case_emb, docs[i:i+1], clinical[i:i+1], pastwork[i:i+1],\n",
    "                        logistics[i:i+1], trust[i:i+1], context\n",
    "                    )['score'].item()\n",
    "                scores.append(s)\n",
    "\n",
    "            scores = np.array(scores)\n",
    "            n5 = ndcg_at_k(scores, labels, 5)\n",
    "            n10 = ndcg_at_k(scores, labels, 10)\n",
    "            ndcg5.append(n5)\n",
    "            ndcg10.append(n10)\n",
    "            context_res.setdefault(ctx, []).append(n5)\n",
    "\n",
    "    overall = {\n",
    "        'ndcg@5': {'mean': float(np.mean(ndcg5)), 'std': float(np.std(ndcg5))},\n",
    "        'ndcg@10': {'mean': float(np.mean(ndcg10)), 'std': float(np.std(ndcg10))}\n",
    "    }\n",
    "    stratified = {k: {'mean': float(np.mean(v)), 'std': float(np.std(v))} for k, v in context_res.items()}\n",
    "    return {'overall': overall, 'stratified': stratified}\n",
    "\n",
    "def ranking_loss(pos_score, neg_score, margin=0.1):\n",
    "    return F.relu(margin - (pos_score - neg_score)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Training Utilities\n",
    "# ============================================================\n",
    "def train_baseline(model, train_loader, val_loader, device, mode='bce', epochs=50, patience=10, lr=1e-4):\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    best_ndcg = -1.0\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    history = {'train_loss': [], 'val_ndcg5': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f'Train {mode} epoch {epoch}', leave=False):\n",
    "            case_emb = batch['case_embedding'].to(device)\n",
    "            pos_doc = batch['pos_doctor_embedding'].to(device)\n",
    "            neg_doc = batch['neg_doctor_embedding'].to(device)\n",
    "            pos_clinical = batch['pos_clinical'].to(device)\n",
    "            neg_clinical = batch['neg_clinical'].to(device)\n",
    "            pos_pastwork = batch['pos_pastwork'].to(device)\n",
    "            neg_pastwork = batch['neg_pastwork'].to(device)\n",
    "            pos_logistics = batch['pos_logistics'].to(device)\n",
    "            neg_logistics = batch['neg_logistics'].to(device)\n",
    "            pos_trust = batch['pos_trust'].to(device)\n",
    "            neg_trust = batch['neg_trust'].to(device)\n",
    "            context = batch['context'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pos_out = model(case_emb, pos_doc, pos_clinical, pos_pastwork, pos_logistics, pos_trust, context)['score']\n",
    "            neg_out = model(case_emb, neg_doc, neg_clinical, neg_pastwork, neg_logistics, neg_trust, context)['score']\n",
    "\n",
    "            if mode == 'bce':\n",
    "                loss = (\n",
    "                    F.binary_cross_entropy(pos_out, torch.ones_like(pos_out)) +\n",
    "                    F.binary_cross_entropy(neg_out, torch.zeros_like(neg_out))\n",
    "                ) / 2.0\n",
    "            else:\n",
    "                loss = ranking_loss(pos_out, neg_out, margin=0.1)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / max(n_batches, 1)\n",
    "        val = evaluate_model(model, val_loader, device, is_mcda=False)\n",
    "        val_ndcg5 = val['overall']['ndcg@5']['mean']\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['val_ndcg5'].append(val_ndcg5)\n",
    "\n",
    "        print(f'Epoch {epoch}: loss={avg_loss:.4f}, val_ndcg5={val_ndcg5:.4f}')\n",
    "\n",
    "        if val_ndcg5 > best_ndcg:\n",
    "            best_ndcg = val_ndcg5\n",
    "            patience_counter = 0\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Run All Baselines\n",
    "# ============================================================\n",
    "all_results = {}\n",
    "\n",
    "print('\\n[1/4] StaticMCDA')\n",
    "mcda = StaticMCDA()\n",
    "all_results['StaticMCDA'] = evaluate_model(mcda, test_loader, device, is_mcda=True)\n",
    "all_results['StaticMCDA']['history'] = {'train_loss': [], 'val_ndcg5': []}\n",
    "\n",
    "print('\\n[2/4] SimpleMLP (BCE)')\n",
    "mlp = SimpleMLP()\n",
    "mlp, mlp_hist = train_baseline(mlp, train_loader, val_loader, device, mode='bce', epochs=50, patience=10, lr=1e-4)\n",
    "torch.save(mlp.state_dict(), '/kaggle/working/results/best_simple_mlp.pt')\n",
    "all_results['SimpleMLP'] = evaluate_model(mlp, test_loader, device, is_mcda=False)\n",
    "all_results['SimpleMLP']['history'] = mlp_hist\n",
    "\n",
    "print('\\n[3/4] NeuralRanker (Ranking Loss)')\n",
    "ranker = NeuralRanker()\n",
    "ranker, ranker_hist = train_baseline(ranker, train_loader, val_loader, device, mode='ranking', epochs=50, patience=10, lr=1e-4)\n",
    "torch.save(ranker.state_dict(), '/kaggle/working/results/best_neural_ranker.pt')\n",
    "all_results['NeuralRanker'] = evaluate_model(ranker, test_loader, device, is_mcda=False)\n",
    "all_results['NeuralRanker']['history'] = ranker_hist\n",
    "\n",
    "print('\\n[4/4] DINModel (Ranking Loss)')\n",
    "din = DINModel()\n",
    "din, din_hist = train_baseline(din, train_loader, val_loader, device, mode='ranking', epochs=50, patience=10, lr=1e-4)\n",
    "torch.save(din.state_dict(), '/kaggle/working/results/best_din_model.pt')\n",
    "all_results['DINModel'] = evaluate_model(din, test_loader, device, is_mcda=False)\n",
    "all_results['DINModel']['history'] = din_hist\n",
    "\n",
    "print('\\nAll baselines finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Save and Summary\n",
    "# ============================================================\n",
    "with open('/kaggle/working/results/baseline_results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print('Saved: /kaggle/working/results/baseline_results.json')\n",
    "print('\\nSummary (NDCG@5 mean ± std):')\n",
    "for name, res in all_results.items():\n",
    "    m = res['overall']['ndcg@5']['mean']\n",
    "    s = res['overall']['ndcg@5']['std']\n",
    "    print(f'  {name}: {m:.4f} ± {s:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
