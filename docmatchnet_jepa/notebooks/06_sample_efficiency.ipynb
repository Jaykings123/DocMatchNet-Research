{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Efficiency Comparison\n",
    "============================\n",
    "Train models with varying data sizes to compare data efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Setup\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_SIZES = [100, 250, 500, 1000, 2500, 5000, 10500]\n",
    "MODELS = ['StaticMCDA', 'NeuralRanker', 'DocMatchNet-Original', 'DocMatchNet-JEPA']\n",
    "SEEDS = [42, 123, 456]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "\n",
    "os.makedirs('/kaggle/working/results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Load Data\n",
    "# ============================================================\n",
    "DATA_DIR = '/kaggle/input/docmatchnet-jepa-data/data'\n",
    "\n",
    "doctor_embeddings = torch.load(f'{DATA_DIR}/doctor_embeddings.pt', weights_only=False)\n",
    "case_embeddings = torch.load(f'{DATA_DIR}/case_embeddings.pt', weights_only=False)\n",
    "clinical_features = torch.load(f'{DATA_DIR}/clinical_features.pt', weights_only=False)\n",
    "pastwork_features = torch.load(f'{DATA_DIR}/pastwork_features.pt', weights_only=False)\n",
    "logistics_features = torch.load(f'{DATA_DIR}/logistics_features.pt', weights_only=False)\n",
    "trust_features = torch.load(f'{DATA_DIR}/trust_features.pt', weights_only=False)\n",
    "context_features = torch.load(f'{DATA_DIR}/context_features.pt', weights_only=False)\n",
    "relevance_labels = torch.load(f'{DATA_DIR}/relevance_labels.pt', weights_only=False)\n",
    "doctor_indices = torch.load(f'{DATA_DIR}/doctor_indices.pt', weights_only=False)\n",
    "splits = torch.load(f'{DATA_DIR}/splits.pt', weights_only=False)\n",
    "\n",
    "TRAIN_FULL = np.array(splits['train'])\n",
    "VAL_IDX = np.array(splits['val'])\n",
    "TEST_IDX = np.array(splits['test'])\n",
    "\n",
    "print(f'Full train size: {len(TRAIN_FULL)}')\n",
    "print(f'Val/Test size: {len(VAL_IDX)}/{len(TEST_IDX)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Dataset Classes\n",
    "# ============================================================\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, indices):\n",
    "        self.indices = np.array(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        case_idx = int(self.indices[idx])\n",
    "        rel = relevance_labels[case_idx]\n",
    "\n",
    "        pos_mask = rel >= 3\n",
    "        if pos_mask.sum() == 0:\n",
    "            pos_mask = rel == rel.max()\n",
    "\n",
    "        neg_mask = rel <= 1\n",
    "        if neg_mask.sum() == 0:\n",
    "            neg_mask = rel < rel.max()\n",
    "\n",
    "        pos_pool = torch.where(pos_mask)[0]\n",
    "        neg_pool = torch.where(neg_mask)[0]\n",
    "\n",
    "        pos_local = pos_pool[torch.randint(len(pos_pool), (1,))].item()\n",
    "        neg_local = neg_pool[torch.randint(len(neg_pool), (1,))].item()\n",
    "\n",
    "        pos_global = doctor_indices[case_idx, pos_local]\n",
    "        neg_global = doctor_indices[case_idx, neg_local]\n",
    "\n",
    "        return {\n",
    "            'case_embedding': case_embeddings[case_idx],\n",
    "            'pos_doctor_embedding': doctor_embeddings[pos_global],\n",
    "            'neg_doctor_embedding': doctor_embeddings[neg_global],\n",
    "            'pos_clinical': clinical_features[case_idx, pos_local],\n",
    "            'neg_clinical': clinical_features[case_idx, neg_local],\n",
    "            'pos_pastwork': pastwork_features[case_idx, pos_local],\n",
    "            'neg_pastwork': pastwork_features[case_idx, neg_local],\n",
    "            'pos_logistics': logistics_features[case_idx, pos_local],\n",
    "            'neg_logistics': logistics_features[case_idx, neg_local],\n",
    "            'pos_trust': trust_features[case_idx, pos_local],\n",
    "            'neg_trust': trust_features[case_idx, neg_local],\n",
    "            'context': context_features[case_idx]\n",
    "        }\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, indices):\n",
    "        self.indices = np.array(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        case_idx = int(self.indices[idx])\n",
    "        gidx = doctor_indices[case_idx]\n",
    "        return {\n",
    "            'case_embedding': case_embeddings[case_idx],\n",
    "            'doctor_embeddings': doctor_embeddings[gidx],\n",
    "            'clinical': clinical_features[case_idx],\n",
    "            'pastwork': pastwork_features[case_idx],\n",
    "            'logistics': logistics_features[case_idx],\n",
    "            'trust': trust_features[case_idx],\n",
    "            'context': context_features[case_idx],\n",
    "            'relevance': relevance_labels[case_idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Model Definitions\n",
    "# ============================================================\n",
    "class StaticMCDA:\n",
    "    def __init__(self):\n",
    "        self.cw = torch.tensor([0.55, 0.20, 0.15, 0.10])\n",
    "        self.pw = torch.tensor([0.30, 0.25, 0.20, 0.15, 0.10])\n",
    "        self.lw = torch.tensor([0.30, 0.25, 0.20, 0.15, 0.10])\n",
    "        self.tw = torch.tensor([0.50, 0.30, 0.20])\n",
    "\n",
    "    def score(self, c, p, l, t):\n",
    "        cw = self.cw.to(c.device, c.dtype)\n",
    "        pw = self.pw.to(c.device, c.dtype)\n",
    "        lw = self.lw.to(c.device, c.dtype)\n",
    "        tw = self.tw.to(c.device, c.dtype)\n",
    "        cscore = (c * cw).sum(-1)\n",
    "        pscore = (p * pw).sum(-1)\n",
    "        lscore = (l * lw).sum(-1)\n",
    "        tscore = (t * tw).sum(-1)\n",
    "        return 0.40 * cscore + 0.25 * pscore + 0.25 * lscore + 0.10 * tscore\n",
    "\n",
    "class NeuralRanker(nn.Module):\n",
    "    def __init__(self, embed_dim=384, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.patient_proj = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.doctor_proj = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.feature_encoder = nn.Sequential(nn.Linear(25, 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.scorer = nn.Sequential(nn.Linear(hidden_dim + 64, 128), nn.ReLU(), nn.Linear(128, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, pe, de, c, p, l, t, ctx):\n",
    "        p1 = self.patient_proj(pe).unsqueeze(1)\n",
    "        d1 = self.doctor_proj(de).unsqueeze(1)\n",
    "        inter, _ = self.cross_attention(p1, d1, d1)\n",
    "        inter = self.layer_norm(inter.squeeze(1))\n",
    "        feat = self.feature_encoder(torch.cat([c, p, l, t, ctx], dim=-1))\n",
    "        return {'score': self.scorer(torch.cat([inter, feat], dim=-1))}\n",
    "\n",
    "class DocMatchNetOriginal(nn.Module):\n",
    "    def __init__(self, embed_dim=384, hidden_dim=256, gate_dim=32):\n",
    "        super().__init__()\n",
    "        self.patient_proj = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.doctor_proj = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.ce = nn.Sequential(nn.Linear(4, gate_dim), nn.ReLU(), nn.Linear(gate_dim, gate_dim), nn.ReLU())\n",
    "        self.pe = nn.Sequential(nn.Linear(5, gate_dim), nn.ReLU(), nn.Linear(gate_dim, gate_dim), nn.ReLU())\n",
    "        self.le = nn.Sequential(nn.Linear(5, gate_dim), nn.ReLU(), nn.Linear(gate_dim, gate_dim), nn.ReLU())\n",
    "        self.te = nn.Sequential(nn.Linear(3, gate_dim), nn.ReLU(), nn.Linear(gate_dim, gate_dim), nn.ReLU())\n",
    "\n",
    "        self.cg = nn.Sequential(nn.Linear(hidden_dim + 8, 64), nn.ReLU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.pg = nn.Sequential(nn.Linear(hidden_dim + 8, 64), nn.ReLU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.lg = nn.Sequential(nn.Linear(hidden_dim + 8, 64), nn.ReLU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.tg = nn.Sequential(nn.Linear(hidden_dim + 8, 64), nn.ReLU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "\n",
    "        self.scorer = nn.Sequential(nn.Linear(gate_dim * 4, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, pe, de, c, p, l, t, ctx):\n",
    "        p1 = self.patient_proj(pe).unsqueeze(1)\n",
    "        d1 = self.doctor_proj(de).unsqueeze(1)\n",
    "        inter, _ = self.cross_attention(p1, d1, d1)\n",
    "        inter = self.norm(inter.squeeze(1))\n",
    "\n",
    "        g_in = torch.cat([inter, ctx], dim=-1)\n",
    "        gc, gp, gl, gt = self.cg(g_in), self.pg(g_in), self.lg(g_in), self.tg(g_in)\n",
    "\n",
    "        fused = torch.cat([gc*self.ce(c), gp*self.pe(p), gl*self.le(l), gt*self.te(t)], dim=-1)\n",
    "        score = self.scorer(fused)\n",
    "        return {'score': score}\n",
    "\n",
    "class DocMatchNetJEPA(nn.Module):\n",
    "    def __init__(self, embed_dim=384, latent_dim=256, gate_dim=32):\n",
    "        super().__init__()\n",
    "        self.patient_encoder = nn.Sequential(nn.Linear(embed_dim, 512), nn.LayerNorm(512), nn.GELU(), nn.Linear(512, latent_dim), nn.LayerNorm(latent_dim))\n",
    "        self.doctor_encoder = nn.Sequential(nn.Linear(embed_dim, 512), nn.LayerNorm(512), nn.GELU(), nn.Linear(512, latent_dim), nn.LayerNorm(latent_dim))\n",
    "\n",
    "        self.ce = nn.Sequential(nn.Linear(4, gate_dim), nn.GELU(), nn.Linear(gate_dim, gate_dim), nn.GELU())\n",
    "        self.pe = nn.Sequential(nn.Linear(5, gate_dim), nn.GELU(), nn.Linear(gate_dim, gate_dim), nn.GELU())\n",
    "        self.le = nn.Sequential(nn.Linear(5, gate_dim), nn.GELU(), nn.Linear(gate_dim, gate_dim), nn.GELU())\n",
    "        self.te = nn.Sequential(nn.Linear(3, gate_dim), nn.GELU(), nn.Linear(gate_dim, gate_dim), nn.GELU())\n",
    "\n",
    "        self.cg = nn.Sequential(nn.Linear(latent_dim + 8, 64), nn.GELU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.pg = nn.Sequential(nn.Linear(latent_dim + 8, 64), nn.GELU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.lg = nn.Sequential(nn.Linear(latent_dim + 8, 64), nn.GELU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.tg = nn.Sequential(nn.Linear(latent_dim + 8, 64), nn.GELU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "\n",
    "        self.predictor = nn.Sequential(nn.Linear(latent_dim + gate_dim * 4, 256), nn.LayerNorm(256), nn.GELU(), nn.Linear(256, latent_dim))\n",
    "        self.pp = nn.Sequential(nn.Linear(latent_dim, latent_dim), nn.GELU(), nn.Linear(latent_dim, 128))\n",
    "        self.dp = nn.Sequential(nn.Linear(latent_dim, latent_dim), nn.GELU(), nn.Linear(latent_dim, 128))\n",
    "        self.log_temperature = nn.Parameter(torch.log(torch.tensor(0.07)))\n",
    "\n",
    "    def forward(self, pe, de, c, p, l, t, ctx):\n",
    "        pl = self.patient_encoder(pe)\n",
    "        dl = self.doctor_encoder(de)\n",
    "\n",
    "        gi = torch.cat([pl, ctx], dim=-1)\n",
    "        gc, gp, gl, gt = self.cg(gi), self.pg(gi), self.lg(gi), self.tg(gi)\n",
    "        feat = torch.cat([gc*self.ce(c), gp*self.pe(p), gl*self.le(l), gt*self.te(t)], dim=-1)\n",
    "\n",
    "        pred = self.predictor(torch.cat([pl, feat], dim=-1))\n",
    "        pred_proj = self.pp(pred)\n",
    "        doc_proj = self.dp(dl)\n",
    "\n",
    "        pn = F.normalize(pred_proj, dim=-1)\n",
    "        dn = F.normalize(doc_proj, dim=-1)\n",
    "        score = ((pn * dn).sum(dim=-1, keepdim=True) + 1) / 2\n",
    "\n",
    "        return {'score': score, 'predicted_ideal': pred_proj, 'doctor_embedding': doc_proj, 'temperature': self.log_temperature.exp()}\n",
    "\n",
    "    def get_parameter_groups(self, base_lr, doctor_lr_mult=0.05):\n",
    "        doc_params = list(self.doctor_encoder.parameters()) + list(self.dp.parameters())\n",
    "        doc_ids = set(id(p) for p in doc_params)\n",
    "        other = [p for p in self.parameters() if id(p) not in doc_ids]\n",
    "        return [{'params': other, 'lr': base_lr}, {'params': doc_params, 'lr': base_lr * doctor_lr_mult}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Train/Eval Utilities\n",
    "# ============================================================\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def ndcg_at_5(scores, labels):\n",
    "    order = np.argsort(-scores)[:5]\n",
    "    dcg = sum((2**labels[i]-1)/np.log2(r+2) for r, i in enumerate(order))\n",
    "    ideal = np.argsort(-labels)[:5]\n",
    "    idcg = sum((2**labels[i]-1)/np.log2(r+2) for r, i in enumerate(ideal))\n",
    "    return float(dcg/idcg) if idcg > 0 else 0.0\n",
    "\n",
    "def evaluate_ndcg5(model_name, model, loader):\n",
    "    if model_name != 'StaticMCDA':\n",
    "        model.eval()\n",
    "\n",
    "    vals = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            pe = batch['case_embedding'].to(device)\n",
    "            docs = batch['doctor_embeddings'].squeeze(0).to(device)\n",
    "            c = batch['clinical'].squeeze(0).to(device)\n",
    "            p = batch['pastwork'].squeeze(0).to(device)\n",
    "            l = batch['logistics'].squeeze(0).to(device)\n",
    "            t = batch['trust'].squeeze(0).to(device)\n",
    "            ctx = batch['context'].to(device)\n",
    "            labels = batch['relevance'].squeeze(0).cpu().numpy()\n",
    "\n",
    "            scores = []\n",
    "            for i in range(docs.shape[0]):\n",
    "                if model_name == 'StaticMCDA':\n",
    "                    s = model.score(c[i:i+1], p[i:i+1], l[i:i+1], t[i:i+1]).item()\n",
    "                else:\n",
    "                    s = model(pe, docs[i:i+1], c[i:i+1], p[i:i+1], l[i:i+1], t[i:i+1], ctx)['score'].item()\n",
    "                scores.append(s)\n",
    "            vals.append(ndcg_at_5(np.array(scores), labels))\n",
    "    return float(np.mean(vals))\n",
    "\n",
    "def train_model(model_name, train_idx, seed):\n",
    "    set_seed(seed)\n",
    "\n",
    "    train_loader = DataLoader(PairDataset(train_idx), batch_size=128, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(EvalDataset(VAL_IDX), batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(EvalDataset(TEST_IDX), batch_size=1, shuffle=False)\n",
    "\n",
    "    if model_name == 'StaticMCDA':\n",
    "        model = StaticMCDA()\n",
    "        return evaluate_ndcg5(model_name, model, test_loader)\n",
    "\n",
    "    if model_name == 'NeuralRanker':\n",
    "        model = NeuralRanker().to(device)\n",
    "        optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    elif model_name == 'DocMatchNet-Original':\n",
    "        model = DocMatchNetOriginal().to(device)\n",
    "        optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    elif model_name == 'DocMatchNet-JEPA':\n",
    "        model = DocMatchNetJEPA().to(device)\n",
    "        optimizer = AdamW(model.get_parameter_groups(1e-4, doctor_lr_mult=0.05), weight_decay=1e-5)\n",
    "    else:\n",
    "        raise ValueError(model_name)\n",
    "\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "    best_val = -1.0\n",
    "    best_state = None\n",
    "    patience = 4\n",
    "    pc = 0\n",
    "\n",
    "    epochs = 12\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            pe = batch['case_embedding'].to(device)\n",
    "            pde = batch['pos_doctor_embedding'].to(device)\n",
    "            nde = batch['neg_doctor_embedding'].to(device)\n",
    "            pcf = batch['pos_clinical'].to(device)\n",
    "            ncf = batch['neg_clinical'].to(device)\n",
    "            ppf = batch['pos_pastwork'].to(device)\n",
    "            npf = batch['neg_pastwork'].to(device)\n",
    "            plf = batch['pos_logistics'].to(device)\n",
    "            nlf = batch['neg_logistics'].to(device)\n",
    "            ptf = batch['pos_trust'].to(device)\n",
    "            ntf = batch['neg_trust'].to(device)\n",
    "            ctx = batch['context'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            po = model(pe, pde, pcf, ppf, plf, ptf, ctx)\n",
    "            no = model(pe, nde, ncf, npf, nlf, ntf, ctx)\n",
    "\n",
    "            if model_name == 'NeuralRanker':\n",
    "                loss = F.relu(0.1 - (po['score'] - no['score'])).mean()\n",
    "            elif model_name == 'DocMatchNet-Original':\n",
    "                rank = F.relu(0.1 - (po['score'] - no['score'])).mean()\n",
    "                bce = (F.binary_cross_entropy(po['score'], torch.ones_like(po['score'])) +\n",
    "                       F.binary_cross_entropy(no['score'], torch.zeros_like(no['score']))) / 2\n",
    "                loss = rank + bce\n",
    "            else:\n",
    "                p = F.normalize(po['predicted_ideal'], dim=-1)\n",
    "                t = F.normalize(po['doctor_embedding'], dim=-1)\n",
    "                logits = p @ t.T / torch.clamp(po['temperature'], min=1e-8)\n",
    "                labels = torch.arange(logits.shape[0], device=logits.device)\n",
    "                loss = (F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels)) / 2\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        val_ndcg = evaluate_ndcg5(model_name, model, val_loader)\n",
    "        if val_ndcg > best_val:\n",
    "            best_val = val_ndcg\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            pc = 0\n",
    "        else:\n",
    "            pc += 1\n",
    "            if pc >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return evaluate_ndcg5(model_name, model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Run Sample-Efficiency Study\n",
    "# ============================================================\n",
    "sample_efficiency_results = {m: {} for m in MODELS}\n",
    "\n",
    "for model_name in MODELS:\n",
    "    print('\\n' + '=' * 70)\n",
    "    print(f'Model: {model_name}')\n",
    "    print('=' * 70)\n",
    "\n",
    "    for data_size in DATA_SIZES:\n",
    "        run_scores = []\n",
    "        print(f'  Data size: {data_size}')\n",
    "\n",
    "        for seed in SEEDS:\n",
    "            set_seed(seed)\n",
    "            n = min(data_size, len(TRAIN_FULL))\n",
    "            rng = np.random.default_rng(seed)\n",
    "            sub_idx = rng.choice(TRAIN_FULL, size=n, replace=False)\n",
    "\n",
    "            score = train_model(model_name, sub_idx, seed)\n",
    "            run_scores.append(float(score))\n",
    "            print(f'    Seed {seed}: NDCG@5={score:.4f}')\n",
    "\n",
    "        sample_efficiency_results[model_name][str(data_size)] = {\n",
    "            'mean': float(np.mean(run_scores)),\n",
    "            'std': float(np.std(run_scores))\n",
    "        }\n",
    "\n",
    "        print(f\"    Mean\u00b1Std: {np.mean(run_scores):.4f} \u00b1 {np.std(run_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Save Results\n",
    "# ============================================================\n",
    "out_path = '/kaggle/working/results/sample_efficiency_results.json'\n",
    "with open(out_path, 'w') as f:\n",
    "    json.dump(sample_efficiency_results, f, indent=2)\n",
    "\n",
    "print(f'Saved: {out_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Learning Curves + JEPA Efficiency Insight\n",
    "# ============================================================\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model_name in MODELS:\n",
    "    ys = [sample_efficiency_results[model_name][str(s)]['mean'] for s in DATA_SIZES]\n",
    "    es = [sample_efficiency_results[model_name][str(s)]['std'] for s in DATA_SIZES]\n",
    "    plt.plot(DATA_SIZES, ys, marker='o', label=model_name)\n",
    "    plt.fill_between(DATA_SIZES, np.array(ys)-np.array(es), np.array(ys)+np.array(es), alpha=0.15)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Training Samples (log scale)')\n",
    "plt.ylabel('NDCG@5')\n",
    "plt.title('Sample Efficiency Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plot_path = '/kaggle/working/results/sample_efficiency_curves.png'\n",
    "plt.savefig(plot_path, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "orig_full = sample_efficiency_results['DocMatchNet-Original'][str(max(DATA_SIZES))]['mean']\n",
    "jepa_curve = {int(k): v['mean'] for k, v in sample_efficiency_results['DocMatchNet-JEPA'].items()}\n",
    "orig_curve = {int(k): v['mean'] for k, v in sample_efficiency_results['DocMatchNet-Original'].items()}\n",
    "\n",
    "def min_size_for_target(curve, target):\n",
    "    for s in sorted(curve.keys()):\n",
    "        if curve[s] >= target:\n",
    "            return s\n",
    "    return None\n",
    "\n",
    "claim = None\n",
    "for pct in [0.95, 0.90, 0.85]:\n",
    "    target = orig_full * pct\n",
    "    j_size = min_size_for_target(jepa_curve, target)\n",
    "    o_size = min_size_for_target(orig_curve, target)\n",
    "    if j_size is not None and o_size is not None and o_size > 0 and j_size < o_size:\n",
    "        reduction = (1.0 - (j_size / o_size)) * 100.0\n",
    "        claim = (pct, target, j_size, o_size, reduction)\n",
    "        break\n",
    "\n",
    "if claim is not None:\n",
    "    pct, target, j_size, o_size, reduction = claim\n",
    "    print('\\nKey Insight:')\n",
    "    print(f\"DocMatchNet-JEPA reaches {pct*100:.0f}% of Original full-data performance (target={target:.4f})\")\n",
    "    print(f\"with {j_size} samples vs {o_size} for Original ({reduction:.1f}% less data).\")\n",
    "else:\n",
    "    print('\\nKey Insight:')\n",
    "    print('No clear data-efficiency crossover found at 95/90/85% thresholds with current runs.')\n",
    "\n",
    "print(f'\\nSaved plot: {plot_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}