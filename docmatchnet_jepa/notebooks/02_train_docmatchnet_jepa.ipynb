{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DocMatchNet-JEPA Training\n",
        "=========================\n",
        "Two-stage training: Alignment Pretraining + Gated SFT\n",
        "\n",
        "GPU Required: T4 or P100  \n",
        "Runtime: ~2-3 hours total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 1: Setup\n",
        "# ============================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Set seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs('/kaggle/working/results', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 2: Load Data\n",
        "# ============================================================\n",
        "DATA_DIR = '/kaggle/input/docmatchnet-data'  # Your saved dataset\n",
        "\n",
        "# Load all tensors\n",
        "doctor_embeddings = torch.load(f'{DATA_DIR}/doctor_embeddings.pt')\n",
        "case_embeddings = torch.load(f'{DATA_DIR}/case_embeddings.pt')\n",
        "clinical_features = torch.load(f'{DATA_DIR}/clinical_features.pt')\n",
        "pastwork_features = torch.load(f'{DATA_DIR}/pastwork_features.pt')\n",
        "logistics_features = torch.load(f'{DATA_DIR}/logistics_features.pt')\n",
        "trust_features = torch.load(f'{DATA_DIR}/trust_features.pt')\n",
        "context_features = torch.load(f'{DATA_DIR}/context_features.pt')\n",
        "relevance_labels = torch.load(f'{DATA_DIR}/relevance_labels.pt')\n",
        "doctor_indices = torch.load(f'{DATA_DIR}/doctor_indices.pt')\n",
        "mcda_scores = torch.load(f'{DATA_DIR}/mcda_scores.pt')\n",
        "case_metadata = torch.load(f'{DATA_DIR}/case_metadata.pt')\n",
        "splits = torch.load(f'{DATA_DIR}/splits.pt')\n",
        "\n",
        "print(f\"Loaded data:\")\n",
        "print(f\"  Doctor embeddings: {doctor_embeddings.shape}\")\n",
        "print(f\"  Case embeddings: {case_embeddings.shape}\")\n",
        "print(f\"  Clinical features: {clinical_features.shape}\")\n",
        "print(f\"  Train/Val/Test: {len(splits['train'])}/{len(splits['val'])}/{len(splits['test'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 3: Dataset Class\n",
        "# ============================================================\n",
        "class DocMatchDatasetJEPA(Dataset):\n",
        "    \"\"\"Dataset for JEPA training with InfoNCE loss.\"\"\"\n",
        "    \n",
        "    def __init__(self, indices, case_emb, doc_emb, doc_indices,\n",
        "                 clinical, pastwork, logistics, trust, context,\n",
        "                 relevance, metadata):\n",
        "        self.indices = indices\n",
        "        self.case_emb = case_emb\n",
        "        self.doc_emb = doc_emb\n",
        "        self.doc_indices = doc_indices\n",
        "        self.clinical = clinical\n",
        "        self.pastwork = pastwork\n",
        "        self.logistics = logistics\n",
        "        self.trust = trust\n",
        "        self.context = context\n",
        "        self.relevance = relevance\n",
        "        self.metadata = metadata\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        case_idx = self.indices[idx]\n",
        "        rel = self.relevance[case_idx]\n",
        "        \n",
        "        # Get positive doctor (relevance >= 3)\n",
        "        pos_mask = rel >= 3\n",
        "        if pos_mask.sum() == 0:\n",
        "            pos_mask = rel >= 2\n",
        "        if pos_mask.sum() == 0:\n",
        "            pos_mask = rel == rel.max()\n",
        "        \n",
        "        pos_local_idx = torch.where(pos_mask)[0]\n",
        "        pos_idx = pos_local_idx[torch.randint(len(pos_local_idx), (1,))].item()\n",
        "        \n",
        "        # Get negative doctor (relevance <= 1)\n",
        "        neg_mask = rel <= 1\n",
        "        if neg_mask.sum() == 0:\n",
        "            neg_mask = rel < rel.max()\n",
        "        \n",
        "        neg_local_idx = torch.where(neg_mask)[0]\n",
        "        neg_idx = neg_local_idx[torch.randint(len(neg_local_idx), (1,))].item()\n",
        "        \n",
        "        # Get global doctor indices\n",
        "        pos_doc_global = self.doc_indices[case_idx, pos_idx]\n",
        "        neg_doc_global = self.doc_indices[case_idx, neg_idx]\n",
        "        \n",
        "        return {\n",
        "            'case_embedding': self.case_emb[case_idx],\n",
        "            'pos_doctor_embedding': self.doc_emb[pos_doc_global],\n",
        "            'neg_doctor_embedding': self.doc_emb[neg_doc_global],\n",
        "            'pos_clinical': self.clinical[case_idx, pos_idx],\n",
        "            'neg_clinical': self.clinical[case_idx, neg_idx],\n",
        "            'pos_pastwork': self.pastwork[case_idx, pos_idx],\n",
        "            'neg_pastwork': self.pastwork[case_idx, neg_idx],\n",
        "            'pos_logistics': self.logistics[case_idx, pos_idx],\n",
        "            'neg_logistics': self.logistics[case_idx, neg_idx],\n",
        "            'pos_trust': self.trust[case_idx, pos_idx],\n",
        "            'neg_trust': self.trust[case_idx, neg_idx],\n",
        "            'context': self.context[case_idx],\n",
        "            'pos_relevance': rel[pos_idx],\n",
        "            'neg_relevance': rel[neg_idx],\n",
        "            'context_category': self.metadata['context_category'][case_idx]\n",
        "        }\n",
        "\n",
        "\n",
        "class DocMatchDatasetEval(Dataset):\n",
        "    \"\"\"Dataset for evaluation (listwise).\"\"\"\n",
        "    \n",
        "    def __init__(self, indices, case_emb, doc_emb, doc_indices,\n",
        "                 clinical, pastwork, logistics, trust, context,\n",
        "                 relevance, metadata):\n",
        "        self.indices = indices\n",
        "        self.case_emb = case_emb\n",
        "        self.doc_emb = doc_emb\n",
        "        self.doc_indices = doc_indices\n",
        "        self.clinical = clinical\n",
        "        self.pastwork = pastwork\n",
        "        self.logistics = logistics\n",
        "        self.trust = trust\n",
        "        self.context = context\n",
        "        self.relevance = relevance\n",
        "        self.metadata = metadata\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        case_idx = self.indices[idx]\n",
        "        doc_global_indices = self.doc_indices[case_idx]\n",
        "        \n",
        "        return {\n",
        "            'case_embedding': self.case_emb[case_idx],\n",
        "            'doctor_embeddings': self.doc_emb[doc_global_indices],\n",
        "            'clinical': self.clinical[case_idx],\n",
        "            'pastwork': self.pastwork[case_idx],\n",
        "            'logistics': self.logistics[case_idx],\n",
        "            'trust': self.trust[case_idx],\n",
        "            'context': self.context[case_idx],\n",
        "            'relevance': self.relevance[case_idx],\n",
        "            'context_category': self.metadata['context_category'][case_idx]\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_ds = DocMatchDatasetJEPA(\n",
        "    splits['train'], case_embeddings, doctor_embeddings, doctor_indices,\n",
        "    clinical_features, pastwork_features, logistics_features, trust_features,\n",
        "    context_features, relevance_labels, case_metadata\n",
        ")\n",
        "\n",
        "val_ds = DocMatchDatasetEval(\n",
        "    splits['val'], case_embeddings, doctor_embeddings, doctor_indices,\n",
        "    clinical_features, pastwork_features, logistics_features, trust_features,\n",
        "    context_features, relevance_labels, case_metadata\n",
        ")\n",
        "\n",
        "test_ds = DocMatchDatasetEval(\n",
        "    splits['test'], case_embeddings, doctor_embeddings, doctor_indices,\n",
        "    clinical_features, pastwork_features, logistics_features, trust_features,\n",
        "    context_features, relevance_labels, case_metadata\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val samples: {len(val_ds)}\")\n",
        "print(f\"Test samples: {len(test_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 4: DocMatchNet-JEPA Model\n",
        "# ============================================================\n",
        "class DocMatchNetJEPA(nn.Module):\n",
        "    \"\"\"DocMatchNet with JEPA-style embedding prediction.\"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim=384, latent_dim=256, gate_dim=32,\n",
        "                 context_dim=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.latent_dim = latent_dim\n",
        "        self.gate_dim = gate_dim\n",
        "        \n",
        "        # Patient Encoder (X-Encoder)\n",
        "        self.patient_encoder = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, latent_dim),\n",
        "            nn.LayerNorm(latent_dim)\n",
        "        )\n",
        "        \n",
        "        # Doctor Encoder (Y-Encoder) - learns slower\n",
        "        self.doctor_encoder = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, latent_dim),\n",
        "            nn.LayerNorm(latent_dim)\n",
        "        )\n",
        "        \n",
        "        # Dimension Encoders\n",
        "        self.clinical_encoder = self._make_encoder(4, gate_dim)\n",
        "        self.pastwork_encoder = self._make_encoder(5, gate_dim)\n",
        "        self.logistics_encoder = self._make_encoder(5, gate_dim)\n",
        "        self.trust_encoder = self._make_encoder(3, gate_dim)\n",
        "        \n",
        "        # Context-Aware Gates\n",
        "        gate_input_dim = latent_dim + context_dim\n",
        "        self.clinical_gate = self._make_gate(gate_input_dim, gate_dim)\n",
        "        self.pastwork_gate = self._make_gate(gate_input_dim, gate_dim)\n",
        "        self.logistics_gate = self._make_gate(gate_input_dim, gate_dim)\n",
        "        self.trust_gate = self._make_gate(gate_input_dim, gate_dim)\n",
        "        \n",
        "        # Initialize gate biases\n",
        "        self._init_gate_biases()\n",
        "        \n",
        "        # Predictor\n",
        "        predictor_input_dim = latent_dim + gate_dim * 4\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(predictor_input_dim, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, latent_dim)\n",
        "        )\n",
        "        \n",
        "        # Projection heads\n",
        "        self.predictor_proj = nn.Sequential(\n",
        "            nn.Linear(latent_dim, latent_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(latent_dim, 128)\n",
        "        )\n",
        "        self.doctor_proj = nn.Sequential(\n",
        "            nn.Linear(latent_dim, latent_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(latent_dim, 128)\n",
        "        )\n",
        "        \n",
        "        # Learnable temperature\n",
        "        self.log_temperature = nn.Parameter(torch.log(torch.tensor(0.07)))\n",
        "        \n",
        "    def _make_encoder(self, input_dim, output_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim),\n",
        "            nn.BatchNorm1d(output_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(output_dim, output_dim),\n",
        "            nn.BatchNorm1d(output_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        \n",
        "    def _make_gate(self, input_dim, output_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(64, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def _init_gate_biases(self):\n",
        "        nn.init.constant_(self.clinical_gate[-2].bias, 0.4)\n",
        "        nn.init.constant_(self.pastwork_gate[-2].bias, 0.0)\n",
        "        nn.init.constant_(self.logistics_gate[-2].bias, 0.0)\n",
        "        nn.init.constant_(self.trust_gate[-2].bias, -0.4)\n",
        "        \n",
        "    def forward(self, patient_emb, doctor_emb, clinical, pastwork,\n",
        "                logistics, trust, context):\n",
        "        # Encode\n",
        "        patient_latent = self.patient_encoder(patient_emb)\n",
        "        doctor_latent = self.doctor_encoder(doctor_emb)\n",
        "        \n",
        "        # Encode features\n",
        "        enc_clinical = self.clinical_encoder(clinical)\n",
        "        enc_pastwork = self.pastwork_encoder(pastwork)\n",
        "        enc_logistics = self.logistics_encoder(logistics)\n",
        "        enc_trust = self.trust_encoder(trust)\n",
        "        \n",
        "        # Compute gates\n",
        "        gate_input = torch.cat([patient_latent, context], dim=-1)\n",
        "        g_clinical = self.clinical_gate(gate_input)\n",
        "        g_pastwork = self.pastwork_gate(gate_input)\n",
        "        g_logistics = self.logistics_gate(gate_input)\n",
        "        g_trust = self.trust_gate(gate_input)\n",
        "        \n",
        "        # Gated fusion\n",
        "        gated_features = torch.cat([\n",
        "            g_clinical * enc_clinical,\n",
        "            g_pastwork * enc_pastwork,\n",
        "            g_logistics * enc_logistics,\n",
        "            g_trust * enc_trust\n",
        "        ], dim=-1)\n",
        "        \n",
        "        # Predict ideal doctor\n",
        "        predictor_input = torch.cat([patient_latent, gated_features], dim=-1)\n",
        "        predicted_ideal = self.predictor(predictor_input)\n",
        "        \n",
        "        # Project\n",
        "        pred_proj = self.predictor_proj(predicted_ideal)\n",
        "        doc_proj = self.doctor_proj(doctor_latent)\n",
        "        \n",
        "        # Score\n",
        "        pred_norm = F.normalize(pred_proj, dim=-1)\n",
        "        doc_norm = F.normalize(doc_proj, dim=-1)\n",
        "        score = (pred_norm * doc_norm).sum(dim=-1, keepdim=True)\n",
        "        score = (score + 1) / 2\n",
        "        \n",
        "        gate_means = {\n",
        "            'clinical': g_clinical.mean(dim=-1),\n",
        "            'pastwork': g_pastwork.mean(dim=-1),\n",
        "            'logistics': g_logistics.mean(dim=-1),\n",
        "            'trust': g_trust.mean(dim=-1)\n",
        "        }\n",
        "        \n",
        "        gates = {\n",
        "            'clinical': g_clinical,\n",
        "            'pastwork': g_pastwork,\n",
        "            'logistics': g_logistics,\n",
        "            'trust': g_trust\n",
        "        }\n",
        "        \n",
        "        return {\n",
        "            'score': score,\n",
        "            'predicted_ideal': pred_proj,\n",
        "            'doctor_embedding': doc_proj,\n",
        "            'gates': gates,\n",
        "            'gate_means': gate_means,\n",
        "            'temperature': self.log_temperature.exp()\n",
        "        }\n",
        "    \n",
        "    def get_parameter_groups(self, base_lr, doctor_lr_mult=0.05):\n",
        "        doctor_params = list(self.doctor_encoder.parameters()) + \\\n",
        "                       list(self.doctor_proj.parameters())\n",
        "        doctor_param_ids = set(id(p) for p in doctor_params)\n",
        "        \n",
        "        other_params = [p for p in self.parameters() \n",
        "                       if id(p) not in doctor_param_ids]\n",
        "        \n",
        "        return [\n",
        "            {'params': other_params, 'lr': base_lr},\n",
        "            {'params': doctor_params, 'lr': base_lr * doctor_lr_mult}\n",
        "        ]\n",
        "    \n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "# Create model\n",
        "model = DocMatchNetJEPA().to(device)\n",
        "print(f\"Model parameters: {model.count_parameters():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 5: Loss Functions\n",
        "# ============================================================\n",
        "class InfoNCELoss(nn.Module):\n",
        "    \"\"\"Bi-directional InfoNCE loss.\"\"\"\n",
        "    \n",
        "    def forward(self, pred_emb, target_emb, temperature):\n",
        "        pred_norm = F.normalize(pred_emb, dim=-1)\n",
        "        target_norm = F.normalize(target_emb, dim=-1)\n",
        "        \n",
        "        logits = pred_norm @ target_norm.T / temperature\n",
        "        \n",
        "        batch_size = logits.shape[0]\n",
        "        labels = torch.arange(batch_size, device=logits.device)\n",
        "        \n",
        "        loss_p2t = F.cross_entropy(logits, labels)\n",
        "        loss_t2p = F.cross_entropy(logits.T, labels)\n",
        "        loss = (loss_p2t + loss_t2p) / 2\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            acc = ((logits.argmax(1) == labels).float().mean() +\n",
        "                   (logits.T.argmax(1) == labels).float().mean()) / 2\n",
        "        \n",
        "        return loss, acc.item()\n",
        "\n",
        "\n",
        "class VICRegGateLoss(nn.Module):\n",
        "    \"\"\"VICReg-style gate regularization.\"\"\"\n",
        "    \n",
        "    def forward(self, gates_dict):\n",
        "        total_loss = 0\n",
        "        \n",
        "        for gate_vals in gates_dict.values():\n",
        "            # Variance loss\n",
        "            var_loss = F.relu(1.0 - gate_vals.var(dim=0)).mean()\n",
        "            \n",
        "            # Covariance loss\n",
        "            if gate_vals.shape[1] > 1:\n",
        "                centered = gate_vals - gate_vals.mean(dim=0)\n",
        "                cov = (centered.T @ centered) / (gate_vals.shape[0] - 1)\n",
        "                off_diag = cov - torch.diag(cov.diag())\n",
        "                cov_loss = (off_diag ** 2).mean()\n",
        "            else:\n",
        "                cov_loss = 0\n",
        "            \n",
        "            total_loss += var_loss + 0.1 * cov_loss\n",
        "        \n",
        "        return total_loss / len(gates_dict)\n",
        "\n",
        "\n",
        "infonce_loss = InfoNCELoss()\n",
        "vicreg_loss = VICRegGateLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 6: Evaluation Function\n",
        "# ============================================================\n",
        "def ndcg_at_k(scores, labels, k):\n",
        "    \"\"\"Compute NDCG@k.\"\"\"\n",
        "    ranking = np.argsort(-scores)[:k]\n",
        "    dcg = sum((2**labels[r] - 1) / np.log2(i + 2) \n",
        "              for i, r in enumerate(ranking))\n",
        "    \n",
        "    ideal_ranking = np.argsort(-labels)[:k]\n",
        "    idcg = sum((2**labels[r] - 1) / np.log2(i + 2) \n",
        "               for i, r in enumerate(ideal_ranking))\n",
        "    \n",
        "    return dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    \"\"\"Evaluate model on dataloader.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    all_ndcg5 = []\n",
        "    all_ndcg10 = []\n",
        "    all_map = []\n",
        "    \n",
        "    context_results = {}\n",
        "    gate_activations = {g: [] for g in ['clinical', 'pastwork', 'logistics', 'trust']}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            case_emb = batch['case_embedding'].to(device)\n",
        "            doc_embs = batch['doctor_embeddings'].squeeze(0).to(device)\n",
        "            clinical = batch['clinical'].squeeze(0).to(device)\n",
        "            pastwork = batch['pastwork'].squeeze(0).to(device)\n",
        "            logistics = batch['logistics'].squeeze(0).to(device)\n",
        "            trust = batch['trust'].squeeze(0).to(device)\n",
        "            context = batch['context'].to(device)\n",
        "            relevance = batch['relevance'].squeeze(0).numpy()\n",
        "            ctx_cat = batch['context_category'][0]\n",
        "            \n",
        "            # Score all doctors\n",
        "            scores = []\n",
        "            for i in range(doc_embs.shape[0]):\n",
        "                output = model(\n",
        "                    case_emb, doc_embs[i:i+1],\n",
        "                    clinical[i:i+1], pastwork[i:i+1],\n",
        "                    logistics[i:i+1], trust[i:i+1],\n",
        "                    context\n",
        "                )\n",
        "                scores.append(output['score'].item())\n",
        "                \n",
        "                # Collect gate activations (from first doctor only)\n",
        "                if i == 0:\n",
        "                    for g_name, g_val in output['gate_means'].items():\n",
        "                        gate_activations[g_name].append(g_val.item())\n",
        "            \n",
        "            scores = np.array(scores)\n",
        "            \n",
        "            # Compute metrics\n",
        "            ndcg5 = ndcg_at_k(scores, relevance, 5)\n",
        "            ndcg10 = ndcg_at_k(scores, relevance, 10)\n",
        "            \n",
        "            all_ndcg5.append(ndcg5)\n",
        "            all_ndcg10.append(ndcg10)\n",
        "            \n",
        "            # Store by context\n",
        "            if ctx_cat not in context_results:\n",
        "                context_results[ctx_cat] = []\n",
        "            context_results[ctx_cat].append(ndcg5)\n",
        "    \n",
        "    results = {\n",
        "        'ndcg@5': (np.mean(all_ndcg5), np.std(all_ndcg5)),\n",
        "        'ndcg@10': (np.mean(all_ndcg10), np.std(all_ndcg10))\n",
        "    }\n",
        "    \n",
        "    stratified = {ctx: (np.mean(vals), np.std(vals)) \n",
        "                  for ctx, vals in context_results.items()}\n",
        "    \n",
        "    gate_stats = {g: np.mean(vals) for g, vals in gate_activations.items()}\n",
        "    \n",
        "    return results, stratified, gate_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 7: Training Loop\n",
        "# ============================================================\n",
        "# Hyperparameters\n",
        "STAGE1_EPOCHS = 20\n",
        "STAGE2_EPOCHS = 40\n",
        "STAGE1_LR = 5e-4\n",
        "STAGE2_LR = 1e-4\n",
        "LAMBDA_GATE = 0.05\n",
        "PATIENCE = 10\n",
        "\n",
        "# Optimizer with differential LR\n",
        "param_groups = model.get_parameter_groups(STAGE2_LR, doctor_lr_mult=0.05)\n",
        "optimizer = AdamW(param_groups, weight_decay=1e-5)\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'infonce_acc': [],\n",
        "    'val_ndcg5': [],\n",
        "    'gate_stats': [],\n",
        "    'temperature': []\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STAGE 1: Alignment Pretraining\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Stage 1: Freeze gates, focus on InfoNCE\n",
        "for name, param in model.named_parameters():\n",
        "    if 'gate' in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "for epoch in range(STAGE1_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    n_batches = 0\n",
        "    \n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
        "        # Move to device\n",
        "        case_emb = batch['case_embedding'].to(device)\n",
        "        pos_doc = batch['pos_doctor_embedding'].to(device)\n",
        "        pos_clinical = batch['pos_clinical'].to(device)\n",
        "        pos_pastwork = batch['pos_pastwork'].to(device)\n",
        "        pos_logistics = batch['pos_logistics'].to(device)\n",
        "        pos_trust = batch['pos_trust'].to(device)\n",
        "        context = batch['context'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(case_emb, pos_doc, pos_clinical, pos_pastwork,\n",
        "                      pos_logistics, pos_trust, context)\n",
        "        \n",
        "        # InfoNCE loss only\n",
        "        loss, acc = infonce_loss(output['predicted_ideal'],\n",
        "                                output['doctor_embedding'],\n",
        "                                output['temperature'])\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc\n",
        "        n_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / n_batches\n",
        "    avg_acc = epoch_acc / n_batches\n",
        "    \n",
        "    history['train_loss'].append(avg_loss)\n",
        "    history['infonce_acc'].append(avg_acc)\n",
        "    history['temperature'].append(output['temperature'].item())\n",
        "    \n",
        "    if epoch % 5 == 0:\n",
        "        val_results, _, gate_stats = evaluate(model, val_loader, device)\n",
        "        history['val_ndcg5'].append(val_results['ndcg@5'][0])\n",
        "        history['gate_stats'].append(gate_stats)\n",
        "        print(f\"Epoch {epoch}: Loss={avg_loss:.4f}, Acc={avg_acc:.4f}, \"\n",
        "              f\"Val NDCG@5={val_results['ndcg@5'][0]:.4f}\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch}: Loss={avg_loss:.4f}, Acc={avg_acc:.4f}\")\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "# Unfreeze gates for Stage 2\n",
        "for name, param in model.named_parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STAGE 2: Gated Supervised Fine-tuning\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_ndcg = 0\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(STAGE2_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_gate_loss = 0\n",
        "    n_batches = 0\n",
        "    \n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
        "        case_emb = batch['case_embedding'].to(device)\n",
        "        pos_doc = batch['pos_doctor_embedding'].to(device)\n",
        "        pos_clinical = batch['pos_clinical'].to(device)\n",
        "        pos_pastwork = batch['pos_pastwork'].to(device)\n",
        "        pos_logistics = batch['pos_logistics'].to(device)\n",
        "        pos_trust = batch['pos_trust'].to(device)\n",
        "        context = batch['context'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(case_emb, pos_doc, pos_clinical, pos_pastwork,\n",
        "                      pos_logistics, pos_trust, context)\n",
        "        \n",
        "        # InfoNCE loss\n",
        "        info_loss, acc = infonce_loss(output['predicted_ideal'],\n",
        "                                     output['doctor_embedding'],\n",
        "                                     output['temperature'])\n",
        "        \n",
        "        # Gate regularization\n",
        "        gate_loss = vicreg_loss(output['gates'])\n",
        "        \n",
        "        # Total loss\n",
        "        loss = info_loss + LAMBDA_GATE * gate_loss\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc\n",
        "        epoch_gate_loss += gate_loss.item()\n",
        "        n_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / n_batches\n",
        "    avg_acc = epoch_acc / n_batches\n",
        "    avg_gate_loss = epoch_gate_loss / n_batches\n",
        "    \n",
        "    history['train_loss'].append(avg_loss)\n",
        "    history['infonce_acc'].append(avg_acc)\n",
        "    history['temperature'].append(output['temperature'].item())\n",
        "    \n",
        "    # Validation\n",
        "    val_results, stratified, gate_stats = evaluate(model, val_loader, device)\n",
        "    val_ndcg = val_results['ndcg@5'][0]\n",
        "    history['val_ndcg5'].append(val_ndcg)\n",
        "    history['gate_stats'].append(gate_stats)\n",
        "    \n",
        "    print(f\"Epoch {epoch}: Loss={avg_loss:.4f}, Acc={avg_acc:.4f}, \"\n",
        "          f\"Gate={avg_gate_loss:.4f}, Val NDCG@5={val_ndcg:.4f}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_ndcg > best_ndcg:\n",
        "        best_ndcg = val_ndcg\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), '/kaggle/working/results/best_jepa_model.pt')\n",
        "        print(f\"  \u2192 New best model saved!\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "    \n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 8: Final Evaluation\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL EVALUATION ON TEST SET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('/kaggle/working/results/best_jepa_model.pt'))\n",
        "\n",
        "test_results, test_stratified, test_gates = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"\\nOverall Results:\")\n",
        "for metric, (mean, std) in test_results.items():\n",
        "    print(f\"  {metric}: {mean:.4f} \u00b1 {std:.4f}\")\n",
        "\n",
        "print(\"\\nStratified Results (NDCG@5):\")\n",
        "for ctx, (mean, std) in test_stratified.items():\n",
        "    print(f\"  {ctx}: {mean:.4f} \u00b1 {std:.4f}\")\n",
        "\n",
        "print(\"\\nGate Activations:\")\n",
        "for gate, val in test_gates.items():\n",
        "    print(f\"  {gate}: {val:.4f}\")\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'overall': {k: {'mean': v[0], 'std': v[1]} \n",
        "                for k, v in test_results.items()},\n",
        "    'stratified': {k: {'mean': v[0], 'std': v[1]} \n",
        "                   for k, v in test_stratified.items()},\n",
        "    'gate_stats': test_gates,\n",
        "    'history': history\n",
        "}\n",
        "\n",
        "with open('/kaggle/working/results/jepa_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"\\n\u2705 Results saved to /kaggle/working/results/jepa_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 9: Visualizations\n",
        "# ============================================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Training loss\n",
        "ax = axes[0, 0]\n",
        "ax.plot(history['train_loss'])\n",
        "ax.axvline(STAGE1_EPOCHS, color='r', linestyle='--', label='Stage 1\u21922')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Training Loss')\n",
        "ax.legend()\n",
        "\n",
        "# InfoNCE accuracy\n",
        "ax = axes[0, 1]\n",
        "ax.plot(history['infonce_acc'])\n",
        "ax.axvline(STAGE1_EPOCHS, color='r', linestyle='--', label='Stage 1\u21922')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('InfoNCE Accuracy')\n",
        "ax.legend()\n",
        "\n",
        "# Validation NDCG@5\n",
        "ax = axes[1, 0]\n",
        "val_epochs = list(range(0, STAGE1_EPOCHS, 5)) + list(range(STAGE1_EPOCHS, len(history['val_ndcg5'])))\n",
        "ax.plot(history['val_ndcg5'])\n",
        "ax.set_xlabel('Checkpoint')\n",
        "ax.set_ylabel('NDCG@5')\n",
        "ax.set_title('Validation NDCG@5')\n",
        "\n",
        "# Gate activations\n",
        "ax = axes[1, 1]\n",
        "contexts = list(test_stratified.keys())\n",
        "gates = ['clinical', 'pastwork', 'logistics', 'trust']\n",
        "x = np.arange(len(gates))\n",
        "ax.bar(x, [test_gates[g] for g in gates])\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(gates, rotation=45)\n",
        "ax.set_ylabel('Mean Activation')\n",
        "ax.set_title('Gate Activations')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/kaggle/working/results/jepa_training_curves.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\u2705 Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 10: Save Embeddings for t-SNE Visualization\n",
        "# ============================================================\n",
        "\n",
        "def save_embeddings_for_visualization(model, test_ds, device,\n",
        "                                       save_path, n_samples=1000):\n",
        "    \"\"\"\n",
        "    Extract and save embeddings from the trained model\n",
        "    for t-SNE visualization in Figure 4.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    patient_latents = []\n",
        "    predicted_ideals = []\n",
        "    doctor_latents = []\n",
        "    specialties = []\n",
        "    contexts = []\n",
        "\n",
        "    n_samples = min(n_samples, len(test_ds))\n",
        "    indices = np.random.choice(len(test_ds), n_samples, replace=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in tqdm(indices, desc=\"Extracting embeddings\"):\n",
        "            sample = test_ds[idx]\n",
        "            case_emb = sample['case_embedding'].unsqueeze(0).to(device)\n",
        "            context = sample['context'].unsqueeze(0).to(device)\n",
        "\n",
        "            # Best matching doctor (highest relevance)\n",
        "            rel = sample['relevance']\n",
        "            best_doc_idx = rel.argmax().item()\n",
        "\n",
        "            doc_emb = sample['doctor_embeddings'][best_doc_idx:best_doc_idx+1].to(device)\n",
        "            clinical = sample['clinical'][best_doc_idx:best_doc_idx+1].to(device)\n",
        "            pastwork = sample['pastwork'][best_doc_idx:best_doc_idx+1].to(device)\n",
        "            logistics = sample['logistics'][best_doc_idx:best_doc_idx+1].to(device)\n",
        "            trust = sample['trust'][best_doc_idx:best_doc_idx+1].to(device)\n",
        "\n",
        "            output = model(case_emb, doc_emb, clinical, pastwork,\n",
        "                           logistics, trust, context)\n",
        "\n",
        "            # Robust extraction: use model outputs when available, otherwise derive from encoders\n",
        "            patient_latent = output.get('patient_latent', model.patient_encoder(case_emb))\n",
        "            doctor_latent = output.get('doctor_latent', model.doctor_encoder(doc_emb))\n",
        "            predicted_ideal = output['predicted_ideal']\n",
        "\n",
        "            patient_latents.append(patient_latent.cpu().numpy())\n",
        "            predicted_ideals.append(predicted_ideal.cpu().numpy())\n",
        "            doctor_latents.append(doctor_latent.cpu().numpy())\n",
        "\n",
        "            case_global_idx = splits['test'][idx]\n",
        "            specialties.append(cases_df.iloc[case_global_idx]['target_specialty'])\n",
        "            contexts.append(sample['context_category'])\n",
        "\n",
        "    embeddings_data = {\n",
        "        'patient_latents': np.concatenate(patient_latents, axis=0),\n",
        "        'predicted_ideals': np.concatenate(predicted_ideals, axis=0),\n",
        "        'doctor_latents': np.concatenate(doctor_latents, axis=0),\n",
        "        'specialties': specialties,\n",
        "        'contexts': contexts\n",
        "    }\n",
        "\n",
        "    torch.save(embeddings_data, save_path)\n",
        "    print(f\"Saved {n_samples} embedding samples to {save_path}\")\n",
        "\n",
        "    return embeddings_data\n",
        "\n",
        "\n",
        "# Run after training is complete\n",
        "embedding_data = save_embeddings_for_visualization(\n",
        "    model, test_ds, device,\n",
        "    '/kaggle/working/results/jepa_embeddings.pt',\n",
        "    n_samples=1000\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}