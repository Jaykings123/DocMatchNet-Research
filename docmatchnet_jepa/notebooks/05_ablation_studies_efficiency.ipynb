{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency Benchmarking\n",
    "=======================\n",
    "Measure inference latency, parameter count, and training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, embed_dim=384):\n",
    "        super().__init__()\n",
    "        input_dim = embed_dim + embed_dim + 4 + 5 + 5 + 3 + 8\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, patient_emb, doctor_emb, clinical, pastwork, logistics, trust, context):\n",
    "        x = torch.cat([patient_emb, doctor_emb, clinical, pastwork, logistics, trust, context], dim=-1)\n",
    "        return self.network(x)\n",
    "\n",
    "class NeuralRanker(nn.Module):\n",
    "    def __init__(self, embed_dim=384, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.patient_proj = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.doctor_proj = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.feature_encoder = nn.Sequential(nn.Linear(25, 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.scorer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + 64, 128), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, patient_emb, doctor_emb, clinical, pastwork, logistics, trust, context):\n",
    "        p = self.patient_proj(patient_emb).unsqueeze(1)\n",
    "        d = self.doctor_proj(doctor_emb).unsqueeze(1)\n",
    "        interaction, _ = self.cross_attention(p, d, d)\n",
    "        interaction = self.layer_norm(interaction.squeeze(1))\n",
    "        feat = self.feature_encoder(torch.cat([clinical, pastwork, logistics, trust, context], dim=-1))\n",
    "        return self.scorer(torch.cat([interaction, feat], dim=-1))\n",
    "\n",
    "class DINModel(nn.Module):\n",
    "    def __init__(self, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.case_encoder = nn.Linear(embed_dim + 8, 128)\n",
    "        self.doctor_encoder = nn.Linear(embed_dim + 17, 128)\n",
    "        self.attention = nn.Sequential(nn.Linear(128 * 3, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "        self.scorer = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, patient_emb, doctor_emb, clinical, pastwork, logistics, trust, context):\n",
    "        case_enc = F.relu(self.case_encoder(torch.cat([patient_emb, context], dim=-1)))\n",
    "        doc_feat = torch.cat([clinical, pastwork, logistics, trust], dim=-1)\n",
    "        doc_enc = F.relu(self.doctor_encoder(torch.cat([doctor_emb, doc_feat], dim=-1)))\n",
    "        attn = torch.sigmoid(self.attention(torch.cat([case_enc, doc_enc, case_enc * doc_enc], dim=-1)))\n",
    "        return self.scorer(attn * doc_enc)\n",
    "\n",
    "class DocMatchNetOriginal(nn.Module):\n",
    "    def __init__(self, embed_dim=384, hidden_dim=256, gate_dim=32):\n",
    "        super().__init__()\n",
    "        self.patient_proj = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.doctor_proj = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ce = nn.Sequential(nn.Linear(4, gate_dim), nn.ReLU(), nn.Linear(gate_dim, gate_dim), nn.ReLU())\n",
    "        self.pe = nn.Sequential(nn.Linear(5, gate_dim), nn.ReLU(), nn.Linear(gate_dim, gate_dim), nn.ReLU())\n",
    "        self.le = nn.Sequential(nn.Linear(5, gate_dim), nn.ReLU(), nn.Linear(gate_dim, gate_dim), nn.ReLU())\n",
    "        self.te = nn.Sequential(nn.Linear(3, gate_dim), nn.ReLU(), nn.Linear(gate_dim, gate_dim), nn.ReLU())\n",
    "        self.cg = nn.Sequential(nn.Linear(hidden_dim + 8, 64), nn.ReLU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.pg = nn.Sequential(nn.Linear(hidden_dim + 8, 64), nn.ReLU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.lg = nn.Sequential(nn.Linear(hidden_dim + 8, 64), nn.ReLU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.tg = nn.Sequential(nn.Linear(hidden_dim + 8, 64), nn.ReLU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.scorer = nn.Sequential(nn.Linear(gate_dim * 4, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, patient_emb, doctor_emb, clinical, pastwork, logistics, trust, context):\n",
    "        p = self.patient_proj(patient_emb).unsqueeze(1)\n",
    "        d = self.doctor_proj(doctor_emb).unsqueeze(1)\n",
    "        interaction, _ = self.cross_attention(p, d, d)\n",
    "        interaction = self.norm(interaction.squeeze(1))\n",
    "        g_in = torch.cat([interaction, context], dim=-1)\n",
    "        gc, gp, gl, gt = self.cg(g_in), self.pg(g_in), self.lg(g_in), self.tg(g_in)\n",
    "        fused = torch.cat([gc*self.ce(clinical), gp*self.pe(pastwork), gl*self.le(logistics), gt*self.te(trust)], dim=-1)\n",
    "        return self.scorer(fused)\n",
    "\n",
    "class DocMatchNetJEPA(nn.Module):\n",
    "    def __init__(self, embed_dim=384, latent_dim=256, gate_dim=32, context_dim=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patient_encoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(512, latent_dim), nn.LayerNorm(latent_dim)\n",
    "        )\n",
    "        self.doctor_encoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(512, latent_dim), nn.LayerNorm(latent_dim)\n",
    "        )\n",
    "        self.ce = nn.Sequential(nn.Linear(4, gate_dim), nn.GELU(), nn.Linear(gate_dim, gate_dim), nn.GELU())\n",
    "        self.pe = nn.Sequential(nn.Linear(5, gate_dim), nn.GELU(), nn.Linear(gate_dim, gate_dim), nn.GELU())\n",
    "        self.le = nn.Sequential(nn.Linear(5, gate_dim), nn.GELU(), nn.Linear(gate_dim, gate_dim), nn.GELU())\n",
    "        self.te = nn.Sequential(nn.Linear(3, gate_dim), nn.GELU(), nn.Linear(gate_dim, gate_dim), nn.GELU())\n",
    "        self.cg = nn.Sequential(nn.Linear(latent_dim + 8, 64), nn.GELU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.pg = nn.Sequential(nn.Linear(latent_dim + 8, 64), nn.GELU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.lg = nn.Sequential(nn.Linear(latent_dim + 8, 64), nn.GELU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.tg = nn.Sequential(nn.Linear(latent_dim + 8, 64), nn.GELU(), nn.Linear(64, gate_dim), nn.Sigmoid())\n",
    "        self.predictor = nn.Sequential(nn.Linear(latent_dim + gate_dim * 4, 256), nn.LayerNorm(256), nn.GELU(), nn.Linear(256, latent_dim))\n",
    "        self.pp = nn.Sequential(nn.Linear(latent_dim, latent_dim), nn.GELU(), nn.Linear(latent_dim, 128))\n",
    "        self.dp = nn.Sequential(nn.Linear(latent_dim, latent_dim), nn.GELU(), nn.Linear(latent_dim, 128))\n",
    "        self.log_temperature = nn.Parameter(torch.log(torch.tensor(0.07)))\n",
    "\n",
    "    def forward(self, patient_emb, doctor_emb, clinical, pastwork, logistics, trust, context):\n",
    "        pl = self.patient_encoder(patient_emb)\n",
    "        dl = self.doctor_encoder(doctor_emb)\n",
    "        gi = torch.cat([pl, context], dim=-1)\n",
    "        gc, gp, gl, gt = self.cg(gi), self.pg(gi), self.lg(gi), self.tg(gi)\n",
    "        feat = torch.cat([gc*self.ce(clinical), gp*self.pe(pastwork), gl*self.le(logistics), gt*self.te(trust)], dim=-1)\n",
    "        pred = self.predictor(torch.cat([pl, feat], dim=-1))\n",
    "        pred_proj = self.pp(pred)\n",
    "        doc_proj = self.dp(dl)\n",
    "        pn = F.normalize(pred_proj, dim=-1)\n",
    "        dn = F.normalize(doc_proj, dim=-1)\n",
    "        score = ((pn * dn).sum(dim=-1, keepdim=True) + 1) / 2\n",
    "        return score\n",
    "\n",
    "def benchmark_model(model, device, n_warmup=50, n_measure=200):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    batch_size = 1\n",
    "    patient_emb = torch.randn(batch_size, 384).to(device)\n",
    "    doctor_emb = torch.randn(batch_size, 384).to(device)\n",
    "    clinical = torch.randn(batch_size, 4).to(device)\n",
    "    pastwork = torch.randn(batch_size, 5).to(device)\n",
    "    logistics = torch.randn(batch_size, 5).to(device)\n",
    "    trust = torch.randn(batch_size, 3).to(device)\n",
    "    context = torch.randn(batch_size, 8).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_warmup):\n",
    "            _ = model(patient_emb, doctor_emb, clinical, pastwork, logistics, trust, context)\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_measure):\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            _ = model(patient_emb, doctor_emb, clinical, pastwork, logistics, trust, context)\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "            times.append((end - start) * 1000)\n",
    "\n",
    "    return {\n",
    "        'mean_ms': float(np.mean(times)),\n",
    "        'std_ms': float(np.std(times)),\n",
    "        'p50_ms': float(np.percentile(times, 50)),\n",
    "        'p95_ms': float(np.percentile(times, 95)),\n",
    "        'p99_ms': float(np.percentile(times, 99))\n",
    "    }\n",
    "\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "models_to_bench = {\n",
    "    'SimpleMLP': SimpleMLP(),\n",
    "    'NeuralRanker': NeuralRanker(),\n",
    "    'DINModel': DINModel(),\n",
    "    'DocMatchNet-Original': DocMatchNetOriginal(),\n",
    "    'DocMatchNet-JEPA': DocMatchNetJEPA()\n",
    "}\n",
    "\n",
    "efficiency_results = {}\n",
    "\n",
    "for model_name, model in models_to_bench.items():\n",
    "    print(f\"\\nBenchmarking {model_name}...\")\n",
    "    total_params, trainable_params = count_parameters(model)\n",
    "\n",
    "    gpu_device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    gpu_results = benchmark_model(model, gpu_device)\n",
    "    cpu_results = benchmark_model(model, torch.device('cpu'))\n",
    "\n",
    "    efficiency_results[model_name] = {\n",
    "        'total_params': int(total_params),\n",
    "        'trainable_params': int(trainable_params),\n",
    "        'gpu_latency_ms': gpu_results,\n",
    "        'cpu_latency_ms': cpu_results\n",
    "    }\n",
    "\n",
    "    print(f\"  Parameters: {trainable_params:,}\")\n",
    "    print(f\"  GPU latency: {gpu_results['mean_ms']:.2f} \u00b1 {gpu_results['std_ms']:.2f} ms\")\n",
    "    print(f\"  CPU latency: {cpu_results['mean_ms']:.2f} \u00b1 {cpu_results['std_ms']:.2f} ms\")\n",
    "\n",
    "print('\\n\\nBatch scoring benchmark (100 doctors per case):')\n",
    "for model_name, model in models_to_bench.items():\n",
    "    model = model.to(device).eval()\n",
    "    patient_emb = torch.randn(1, 384).to(device)\n",
    "    context = torch.randn(1, 8).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        for _ in range(100):\n",
    "            doctor_emb = torch.randn(1, 384).to(device)\n",
    "            clinical = torch.randn(1, 4).to(device)\n",
    "            pastwork = torch.randn(1, 5).to(device)\n",
    "            logistics = torch.randn(1, 5).to(device)\n",
    "            trust = torch.randn(1, 3).to(device)\n",
    "            _ = model(patient_emb, doctor_emb, clinical, pastwork, logistics, trust, context)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "\n",
    "    batch_time = (end - start) * 1000\n",
    "    print(f\"  {model_name}: {batch_time:.1f} ms for 100 doctors\")\n",
    "\n",
    "with open('/kaggle/working/results/efficiency_results.json', 'w') as f:\n",
    "    json.dump(efficiency_results, f, indent=2, default=str)\n",
    "\n",
    "print('\\n\u2705 Efficiency results saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}